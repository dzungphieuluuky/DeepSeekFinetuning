{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dzungphieuluuky/DeepSeekFinetuning/blob/main/Deepseek_OCR_(3B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HDb4OyZjzSX"
      },
      "source": [
        "### Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RGaqHGUjzSY"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os, re\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth\n",
        "else:\n",
        "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
        "    import torch; v = re.match(r\"[0-9]{1,}\\.[0-9]{1,}\", str(torch.__version__)).group(0)\n",
        "    xformers = \"xformers==\" + (\"0.0.33.post1\" if v==\"2.9\" else \"0.0.32.post2\" if v==\"2.8\" else \"0.0.29.post3\")\n",
        "    !pip install --no-deps bitsandbytes accelerate {xformers} peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf \"datasets==4.3.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
        "    !pip install --no-deps unsloth\n",
        "!pip install transformers==4.56.2\n",
        "!pip install --no-deps trl==0.22.2\n",
        "!pip install jiwer\n",
        "!pip install einops addict easydict\n",
        "!pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo transformers timm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Environment Inspection\n",
        "import os\n",
        "import sys\n",
        "import platform\n",
        "import subprocess\n",
        "\n",
        "# --- Environment Detection and Path Setup ---\n",
        "\n",
        "def detect_env_and_setup_paths():\n",
        "    \"\"\"\n",
        "    Detects the current environment (Kaggle, Colab, or Local) and\n",
        "    returns appropriate paths and environment name.\n",
        "    \"\"\"\n",
        "    env_name = \"Local/Other\"\n",
        "    input_dir = \"input/\"\n",
        "    output_dir = \"output/\"\n",
        "\n",
        "    # 1. Detect Kaggle\n",
        "    if 'KAGGLE_KERNEL_RUN_TYPE' in os.environ:\n",
        "        env_name = \"Kaggle\"\n",
        "        # Kaggle's standard paths\n",
        "        input_dir = \"/kaggle/input/\"\n",
        "        output_dir = \"/kaggle/working/\"\n",
        "\n",
        "    # 2. Detect Colab (Note: Colab also has 'COLAB_GPU' if a GPU is assigned)\n",
        "    elif 'google.colab' in sys.modules:\n",
        "        env_name = \"Colab\"\n",
        "        # Colab's default paths relative to the content folder\n",
        "        input_dir = \"/content/\" # Often you mount Drive or download here\n",
        "        output_dir = \"/content/\" # Default working directory\n",
        "        print(\"Note: In Colab, you might need to mount Google Drive manually for persistent storage:\")\n",
        "        print(\"      from google.colab import drive; drive.mount('/content/drive')\")\n",
        "\n",
        "    # 3. Default to Local/Other\n",
        "    # Paths are set to a simple structure relative to the script location\n",
        "\n",
        "    return env_name, input_dir, output_dir\n",
        "\n",
        "# --- System and Package Info ---\n",
        "\n",
        "def get_system_info(env_name):\n",
        "    \"\"\"Prints Python, PyTorch, CUDA, and GPU information.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(f\"       ðŸ’» System and Package Information for {env_name}\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # 1. Python Version\n",
        "    print(f\"**Python Version:** {sys.version.split()[0]} ({platform.python_implementation()})\")\n",
        "\n",
        "    # 2. PyTorch and CUDA Info\n",
        "    try:\n",
        "        import torch\n",
        "        print(f\"**PyTorch Version:** {torch.__version__}\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            print(\"\\n**CUDA/GPU Information (PyTorch):**\")\n",
        "            # CUDA version\n",
        "            print(f\"  - CUDA is Available: **True**\")\n",
        "            print(f\"  - CUDA Version (Runtime): {torch.version.cuda}\")\n",
        "            # GPU details\n",
        "            gpu_count = torch.cuda.device_count()\n",
        "            print(f\"  - GPU Count: {gpu_count}\")\n",
        "            for i in range(gpu_count):\n",
        "                print(f\"  - Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "        else:\n",
        "            print(f\"  - CUDA is Available: **False** (Running on CPU)\")\n",
        "    except ImportError:\n",
        "        print(\"\\n**PyTorch:** Not installed or not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n**PyTorch/CUDA Check Error:** {e}\")\n",
        "\n",
        "    # 3. nvidia-smi (System-level GPU info)\n",
        "    print(\"\\n**NVIDIA-SMI Output (Raw Driver/System Info):**\")\n",
        "    try:\n",
        "        # Run nvidia-smi command\n",
        "        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True, check=True)\n",
        "        print(result.stdout)\n",
        "    except FileNotFoundError:\n",
        "        # This will happen if nvidia-smi is not in PATH or no NVIDIA driver is installed\n",
        "        print(\"  - `nvidia-smi` command not found (No NVIDIA GPU/drivers or not in PATH).\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        # This might happen if the command runs but fails (e.g., driver issues)\n",
        "        print(f\"  - `nvidia-smi` failed to run. Error: {e.stderr.strip()}\")\n",
        "\n",
        "# --- Main Execution ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Get environment details and paths\n",
        "    env_name, INPUT_DIR, OUTPUT_DIR = detect_env_and_setup_paths()\n",
        "\n",
        "    # Print detected environment and paths\n",
        "    print(\"=\"*50)\n",
        "    print(f\"       âœ… Environment Detected: **{env_name}**\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"**Input Path (Default):** {INPUT_DIR}\")\n",
        "    print(f\"**Output Path (Default):** {OUTPUT_DIR}\")\n",
        "\n",
        "    # Run system checks\n",
        "    get_system_info(env_name)\n",
        "\n",
        "    print(\"=\"*50)\n",
        "\n",
        "# Example Usage within the script (simulated)\n",
        "# train_data_path = os.path.join(INPUT_DIR, \"dataset_folder\", \"train.csv\")\n",
        "# model_save_path = os.path.join(OUTPUT_DIR, \"best_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Sbh0Cg7cTvz",
        "outputId": "958c7303-4c34-49bb-ff60-fee6237cbe70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Note: In Colab, you might need to mount Google Drive manually for persistent storage:\n",
            "      from google.colab import drive; drive.mount('/content/drive')\n",
            "==================================================\n",
            "       âœ… Environment Detected: **Colab**\n",
            "==================================================\n",
            "**Input Path (Default):** /content/\n",
            "**Output Path (Default):** /content/\n",
            "\n",
            "==================================================\n",
            "       ðŸ’» System and Package Information for Colab\n",
            "==================================================\n",
            "**Python Version:** 3.12.12 (CPython)\n",
            "**PyTorch Version:** 2.9.0+cu126\n",
            "\n",
            "**CUDA/GPU Information (PyTorch):**\n",
            "  - CUDA is Available: **True**\n",
            "  - CUDA Version (Runtime): 12.6\n",
            "  - GPU Count: 1\n",
            "  - Device 0: Tesla T4\n",
            "\n",
            "**NVIDIA-SMI Output (Raw Driver/System Info):**\n",
            "Sun Dec 14 17:43:28 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P8             11W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Unzipping everything\n",
        "import os\n",
        "import glob\n",
        "from zipfile import ZipFile\n",
        "\n",
        "zip_file_paths = glob.glob(os.path.join(INPUT_DIR, '*.zip'))\n",
        "if not zip_file_paths:\n",
        "    print(f'No .zip files found in {INPUT_DIR}.')\n",
        "else:\n",
        "    for zip_file_path in zip_file_paths:\n",
        "        if os.path.exists(zip_file_path):\n",
        "            print(f'Unzipping {zip_file_path}...')\n",
        "            !unzip -q -o {zip_file_path} -d ./\n",
        "\n",
        "            print(f'Unzipping of {zip_file_path} complete.')\n",
        "        else:\n",
        "            print(f'Error: The file {zip_file_path} was not found (post-glob check).')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzcqvtawcZWO",
        "outputId": "f4a67176-0aa6-4484-b545-e10baacf7b95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No .zip files found in /content/.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure environment"
      ],
      "metadata": {
        "id": "LPZt-53PteZ3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xLDGk41C7IF"
      },
      "source": [
        "### Unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtBzcx2RjzSZ"
      },
      "source": [
        "Let's prepare the OCR model to our local first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tne3ExsBjzSZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "2a0543da62e84b268f4d3bb97452609c",
            "59d29e498f2d4aa4aad7706324d55e03",
            "f049f92b4b434a8893e7c6be5bfc0545",
            "cc21e78711624a42b1d914f5e157a310",
            "684ccf2f7f934aa8aabfa4fcff16235a",
            "15a7f8f0065647e4995b1b6b52e7bc8e",
            "2ba4665432b64573b87f2611ef903802",
            "f5f9e13cdda9434eb205bcfc8823e6ae",
            "1b26c0b53d7a49768b85f782c1c20874",
            "187c66983ec24d8ba95a60af28f6c467",
            "d92b944f355e4a98a4ef54eb6360c893"
          ]
        },
        "outputId": "c5bd9ef3-cf79-4d31-aaee-cd85e0a7aeed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 21 files:   0%|          | 0/21 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a0543da62e84b268f4d3bb97452609c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/deepseek_ocr'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "snapshot_download(\"unsloth/DeepSeek-OCR\", local_dir = \"deepseek_ocr\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dzungphieuluuky/DeepSeekFinetuning.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmekqwCPxzjv",
        "outputId": "5ce78595-2259-4efb-950d-b17f2f82c2f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DeepSeekFinetuning' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QmUBVEnvCDJv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63779385-9175-45b2-c300-d4e8a0964ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "Flax classes are deprecated and will be removed in Diffusers v1.0.0. We recommend migrating to PyTorch classes or pinning your version of Diffusers.\n",
            "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: WARNING `trust_remote_code` is True.\n",
            "Are you certain you want to do remote code execution?\n",
            "==((====))==  Unsloth 2025.12.5: Fast Deepseekocr patching. Transformers: 4.57.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.9.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.5.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post1. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n",
            "You are using a model of type deepseek_vl_v2 to instantiate a model of type DeepseekOCR. This is not supported for all configurations of models and can yield errors.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DeepseekOCRForCausalLM were not initialized from the model checkpoint at ./deepseek_ocr and are newly initialized: ['model.vision_model.embeddings.position_ids']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "# @title Download original DeepSeek OCR model\n",
        "import unsloth\n",
        "from unsloth import FastVisionModel # FastLanguageModel for LLMs\n",
        "import torch\n",
        "from transformers import AutoModel\n",
        "import os\n",
        "os.environ[\"UNSLOTH_WARN_UNINITIALIZED\"] = '0'\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/Qwen3-VL-8B-Instruct-bnb-4bit\", # Qwen 3 vision support\n",
        "    \"unsloth/Qwen3-VL-8B-Thinking-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-VL-32B-Instruct-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-VL-32B-Thinking-bnb-4bit\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "original_model, original_tokenizer = FastVisionModel.from_pretrained(\n",
        "    \"./deepseek_ocr\",\n",
        "    load_in_4bit = False, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "    auto_model = AutoModel,\n",
        "    trust_remote_code=True,\n",
        "    unsloth_force_compile=True,\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9CBpiISFa6C"
      },
      "source": [
        "To format the dataset, all vision finetuning tasks should be formatted as follows:\n",
        "\n",
        "```python\n",
        "[\n",
        "{ \"role\": \"<|User|>\",\n",
        "  \"content\": \"\",\n",
        "  \"images\": []\n",
        "},\n",
        "{ \"role\": \"<|Assistant|>\",\n",
        "  \"content\": \"\"\n",
        "},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Load datasets using Hugging Face load_dataset\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, DatasetDict, Dataset\n",
        "import os\n",
        "import json\n",
        "from PIL import Image as PILImage\n",
        "\n",
        "# --- Configuration ---\n",
        "INPUT_DIR = \"/content\"  # Update this with your actual INPUT_DIR\n",
        "DATA_DIR = os.path.join(INPUT_DIR, \"DeepSeekFinetuning/UIT_HWDB_line\")\n",
        "\n",
        "def load_and_format_datasets(\n",
        "    data_dir,\n",
        "    train_labels_file=None,\n",
        "    test_labels_file=None,\n",
        "    instruction=\"<image>\\nFree OCR.\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Load and format train/test datasets using Hugging Face's load_dataset.\n",
        "\n",
        "    Args:\n",
        "        data_dir: Base directory containing train_data and test_data folders\n",
        "        train_labels_file: Path to train labels.json (optional, defaults to data_dir/train_data/labels.json)\n",
        "        test_labels_file: Path to test labels.json (optional, defaults to data_dir/test_data/labels.json)\n",
        "        instruction: Instruction prompt for the OCR task\n",
        "\n",
        "    Returns:\n",
        "        DatasetDict with 'train' and 'test' splits in DeepSeek conversation format\n",
        "    \"\"\"\n",
        "\n",
        "    # Set default paths if not provided\n",
        "    if train_labels_file is None:\n",
        "        train_labels_file = os.path.join(data_dir, \"train_data\", \"labels.json\")\n",
        "    if test_labels_file is None:\n",
        "        test_labels_file = os.path.join(data_dir, \"test_data\", \"labels.json\")\n",
        "\n",
        "    train_data_dir = os.path.dirname(train_labels_file)\n",
        "    test_data_dir = os.path.dirname(test_labels_file)\n",
        "\n",
        "    print(f\"Loading datasets from:\")\n",
        "    print(f\"  Train data: {train_data_dir}\")\n",
        "    print(f\"  Train labels: {train_labels_file}\")\n",
        "    print(f\"  Test data: {test_data_dir}\")\n",
        "    print(f\"  Test labels: {test_labels_file}\")\n",
        "\n",
        "    def create_csv_from_labels(data_dir, labels_file, split_name):\n",
        "        \"\"\"Helper to create CSV file from labels.json\"\"\"\n",
        "        print(f\"\\nProcessing {split_name} split...\")\n",
        "\n",
        "        # Load labels\n",
        "        with open(labels_file, 'r', encoding='utf-8') as f:\n",
        "            labels_data = json.load(f)\n",
        "\n",
        "        # Prepare data\n",
        "        image_paths = []\n",
        "        texts = []\n",
        "        missing_count = 0\n",
        "\n",
        "        for filename, text in labels_data.items():\n",
        "            full_path = os.path.join(data_dir, filename)\n",
        "\n",
        "            if os.path.exists(full_path):\n",
        "                image_paths.append(full_path)\n",
        "                texts.append(text)\n",
        "            else:\n",
        "                missing_count += 1\n",
        "                if missing_count <= 3:\n",
        "                    print(f\"  Warning: Missing file {filename}\")\n",
        "\n",
        "        print(f\"  Found {len(image_paths)} valid samples, {missing_count} missing files\")\n",
        "\n",
        "        if len(image_paths) == 0:\n",
        "            raise ValueError(f\"No valid images found for {split_name} split!\")\n",
        "\n",
        "        # Create DataFrame\n",
        "        df = pd.DataFrame({\n",
        "            'image_path': image_paths,\n",
        "            'text': texts\n",
        "        })\n",
        "\n",
        "        # Save to temporary CSV\n",
        "        import tempfile\n",
        "        temp_csv = f\"/tmp/{split_name}_data.csv\"\n",
        "        df.to_csv(temp_csv, index=False)\n",
        "\n",
        "        return temp_csv\n",
        "\n",
        "    # Create CSV files\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    train_csv = create_csv_from_labels(train_data_dir, train_labels_file, \"train\")\n",
        "    test_csv = create_csv_from_labels(test_data_dir, test_labels_file, \"test\")\n",
        "\n",
        "    # Load datasets using load_dataset\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Loading datasets with load_dataset...\")\n",
        "\n",
        "    train_ds = load_dataset(\"csv\", data_files=train_csv, split=\"train\")\n",
        "    test_ds = load_dataset(\"csv\", data_files=test_csv, split=\"test\")\n",
        "\n",
        "    print(f\"Train: {len(train_ds)} samples\")\n",
        "    print(f\"Test: {len(test_ds)} samples\")\n",
        "\n",
        "    # Convert to conversation format\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Converting to DeepSeek conversation format...\")\n",
        "\n",
        "    def convert_to_conversation(sample):\n",
        "        \"\"\"Convert sample to conversation format with PIL Image\"\"\"\n",
        "        pil_image = PILImage.open(sample['image_path']).convert('RGB')\n",
        "\n",
        "        conversation = [\n",
        "            {\n",
        "                \"role\": \"<|User|>\",\n",
        "                \"content\": instruction,\n",
        "                \"images\": [pil_image]\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"<|Assistant|>\",\n",
        "                \"content\": sample[\"text\"]\n",
        "            },\n",
        "        ]\n",
        "        return {\"messages\": conversation}\n",
        "\n",
        "    # Convert datasets\n",
        "    train_converted = [convert_to_conversation(sample) for sample in train_ds]\n",
        "    test_converted = [convert_to_conversation(sample) for sample in test_ds]\n",
        "\n",
        "    # Create final Dataset objects\n",
        "    train_dataset = Dataset.from_list(train_converted)\n",
        "    test_dataset = Dataset.from_list(test_converted)\n",
        "\n",
        "    # Create DatasetDict\n",
        "    dataset_dict = DatasetDict({\n",
        "        \"train\": train_dataset,\n",
        "        \"test\": test_dataset\n",
        "    })\n",
        "\n",
        "    # Clean up temporary files\n",
        "    if os.path.exists(train_csv):\n",
        "        os.remove(train_csv)\n",
        "    if os.path.exists(test_csv):\n",
        "        os.remove(test_csv)\n",
        "\n",
        "    # Display summary\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Dataset Summary:\")\n",
        "    print(f\"Train samples: {len(dataset_dict['train'])}\")\n",
        "    print(f\"Test samples: {len(dataset_dict['test'])}\")\n",
        "\n",
        "    # Show example\n",
        "    print(\"\\nExample from train dataset:\")\n",
        "    sample = dataset_dict['train'][0]\n",
        "    print(f\"Keys: {list(sample.keys())}\")\n",
        "    print(f\"Messages: {len(sample['messages'])}\")\n",
        "    print(f\"User role: {sample['messages'][0]['role']}\")\n",
        "    print(f\"User content: {sample['messages'][0]['content']}\")\n",
        "    print(f\"Assistant content: {sample['messages'][1]['content'][:50]}...\")\n",
        "    print(f\"Image type: {type(sample['messages'][0]['images'][0])}\")\n",
        "    print(f\"Image size: {sample['messages'][0]['images'][0].size}\")\n",
        "\n",
        "    return dataset_dict"
      ],
      "metadata": {
        "id": "ccHuAue-kg0J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 498,
          "referenced_widgets": [
            "64b88451471f41fab2c3675e9d07e7e3",
            "79f06f9e152a4f308f94e5e8f9de3cfe",
            "7378120267234b6485e36b9c207c8937",
            "6210f0ed6dab4f70a58b505df0c960b3",
            "f815c30ee1ca4557a3dd57cda596b9b4",
            "b18b7a7475964cb491a15ce08e1c4315",
            "8b3dd99568084b6ab8448b52524d858a",
            "ef986553433944ffa505e56c6fc55485",
            "0fa784ecaa68483b8b4a24bb235547bd",
            "00286b23f2c24eee8c57a9fec06e8183",
            "efd7ad991d8046f6a3182cdcd61d4619",
            "2dd252dc38c74311882f785030865e95",
            "de3e1f7404d741b1aefa4df3b4d01d72",
            "b7aa55894d734197bdd5bc638008ff46",
            "2615ada100ad4afaa058cfc939ed2140",
            "d267935cdad9441ca33764b4b153e7b5",
            "0b86ab892923435ba0cf2da603157c0c",
            "5195ecaa378a4206841b70d6ac22a227",
            "7f521159dc8945d183819d70a7a2220f",
            "815ea3d114094caba231aa1e299eecf0",
            "ee717c91473f4a6eaa21cf802ad84e6c",
            "c99a754f7b9642729a0423f6508bd890"
          ]
        },
        "outputId": "4c91e30d-8169-4056-c1af-84d4c5a82fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading datasets from:\n",
            "  Train data: /content/DeepSeekFinetuning/UIT_HWDB_line/train_data\n",
            "  Train labels: /content/DeepSeekFinetuning/UIT_HWDB_line/train_data/labels.json\n",
            "  Test data: /content/DeepSeekFinetuning/UIT_HWDB_line/test_data\n",
            "  Test labels: /content/DeepSeekFinetuning/UIT_HWDB_line/test_data/labels.json\n",
            "\n",
            "==================================================\n",
            "\n",
            "Processing train split...\n",
            "  Warning: Missing file 10_1.jpg\n",
            "  Warning: Missing file 100_1.jpg\n",
            "  Warning: Missing file 100_2.jpg\n",
            "  Found 4368 valid samples, 2660 missing files\n",
            "\n",
            "Processing test split...\n",
            "  Found 201 valid samples, 0 missing files\n",
            "\n",
            "==================================================\n",
            "Loading datasets with load_dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "64b88451471f41fab2c3675e9d07e7e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2dd252dc38c74311882f785030865e95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 4368 samples\n",
            "Test: 201 samples\n",
            "\n",
            "==================================================\n",
            "Converting to DeepSeek conversation format...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = load_and_format_datasets(DATA_DIR)\n",
        "\n",
        "# Access train and test datasets\n",
        "train_dataset = datasets[\"train\"]\n",
        "test_dataset = datasets[\"test\"]\n",
        "\n",
        "print(f\"\\nFinal dataset sizes:\")\n",
        "print(f\"Train: {len(train_dataset)}\")\n",
        "print(f\"Test: {len(test_dataset)}\")"
      ],
      "metadata": {
        "id": "Mwf6L2mLiqQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[1415]\n",
        "test_dataset[150]"
      ],
      "metadata": {
        "id": "-qzb19kZbFrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfZVh2ByxFNh"
      },
      "source": [
        "### Let's Evaluate Deepseek-OCR Baseline Performance on UIT Handwritten Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZHjLlxrI_TH"
      },
      "outputs": [],
      "source": [
        "# Save an image that will not be used during training for evaluation purposes\n",
        "train_dataset[1415]['image'].save(\"your_image.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68O9GTfTXfHo"
      },
      "outputs": [],
      "source": [
        "train_dataset[1415]['image']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzckMII_02s_"
      },
      "source": [
        "# Let's finetune Deepseek-OCR !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters.\n",
        "\n",
        "**[NEW]** We also support finetuning ONLY the vision part of the model, or ONLY the language part. Or you can select both! You can also select to finetune the attention or the MLP layers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "# @title Get finetuned model initialization with PEFT\n",
        "finetuned_model = FastVisionModel.get_peft_model(\n",
        "    original_model,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "    ],\n",
        "\n",
        "    r = 16,           # The larger, the higher the accuracy, but might overfit\n",
        "    lora_alpha = 16,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,  # We support rank stabilized LoRA\n",
        "    loftq_config = None, # And LoftQ\n",
        "    # target_modules = \"all-linear\", # Optional now! Can specify a list if needed\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGFzmplrEy9I"
      },
      "outputs": [],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "E2WR-p20LcG_"
      },
      "outputs": [],
      "source": [
        "# @title Create datacollator\n",
        "\n",
        "import torch\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Any, Tuple\n",
        "from PIL import Image, ImageOps\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import io\n",
        "\n",
        "from deepseek_ocr.modeling_deepseekocr import (\n",
        "    format_messages,\n",
        "    text_encode,\n",
        "    BasicImageTransform,\n",
        "    dynamic_preprocess,\n",
        ")\n",
        "\n",
        "@dataclass\n",
        "class DeepSeekOCRDataCollator:\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        tokenizer: Tokenizer\n",
        "        model: Model\n",
        "        image_size: Size for image patches (default: 640)\n",
        "        base_size: Size for global view (default: 1024)\n",
        "        crop_mode: Whether to use dynamic cropping for large images\n",
        "        train_on_responses_only: If True, only train on assistant responses (mask user prompts)\n",
        "    \"\"\"\n",
        "    tokenizer: Any\n",
        "    model: Any\n",
        "    image_size: int = 640\n",
        "    base_size: int = 1024\n",
        "    crop_mode: bool = True\n",
        "    image_token_id: int = 128815\n",
        "    train_on_responses_only: bool = True\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer,\n",
        "        model,\n",
        "        image_size: int = 640,\n",
        "        base_size: int = 1024,\n",
        "        crop_mode: bool = True,\n",
        "        train_on_responses_only: bool = True,\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model = model\n",
        "        self.image_size = image_size\n",
        "        self.base_size = base_size\n",
        "        self.crop_mode = crop_mode\n",
        "        self.image_token_id = 128815\n",
        "        self.dtype = model.dtype  # Get dtype from model\n",
        "        self.train_on_responses_only = train_on_responses_only\n",
        "\n",
        "        self.image_transform = BasicImageTransform(\n",
        "            mean=(0.5, 0.5, 0.5),\n",
        "            std=(0.5, 0.5, 0.5),\n",
        "            normalize=True\n",
        "        )\n",
        "        self.patch_size = 16\n",
        "        self.downsample_ratio = 4\n",
        "\n",
        "        # Get BOS token ID from tokenizer\n",
        "        if hasattr(tokenizer, 'bos_token_id') and tokenizer.bos_token_id is not None:\n",
        "            self.bos_id = tokenizer.bos_token_id\n",
        "        else:\n",
        "            self.bos_id = 0\n",
        "            print(f\"Warning: tokenizer has no bos_token_id, using default: {self.bos_id}\")\n",
        "\n",
        "    def deserialize_image(self, image_data) -> Image.Image:\n",
        "        \"\"\"Convert image data (bytes dict or PIL Image) to PIL Image in RGB mode\"\"\"\n",
        "        if isinstance(image_data, Image.Image):\n",
        "            return image_data.convert(\"RGB\")\n",
        "        elif isinstance(image_data, dict) and 'bytes' in image_data:\n",
        "            image_bytes = image_data['bytes']\n",
        "            image = Image.open(io.BytesIO(image_bytes))\n",
        "            return image.convert(\"RGB\")\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported image format: {type(image_data)}\")\n",
        "\n",
        "    def calculate_image_token_count(self, image: Image.Image, crop_ratio: Tuple[int, int]) -> int:\n",
        "        \"\"\"Calculate the number of tokens this image will generate\"\"\"\n",
        "        num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n",
        "        num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "\n",
        "        width_crop_num, height_crop_num = crop_ratio\n",
        "\n",
        "        if self.crop_mode:\n",
        "            img_tokens = num_queries_base * num_queries_base + 1\n",
        "            if width_crop_num > 1 or height_crop_num > 1:\n",
        "                img_tokens += (num_queries * width_crop_num + 1) * (num_queries * height_crop_num)\n",
        "        else:\n",
        "            img_tokens = num_queries * num_queries + 1\n",
        "\n",
        "        return img_tokens\n",
        "\n",
        "    def process_image(self, image: Image.Image) -> Tuple[List, List, List, List, Tuple[int, int]]:\n",
        "        \"\"\"\n",
        "        Process a single image based on crop_mode and size thresholds\n",
        "\n",
        "        Returns:\n",
        "            Tuple of (images_list, images_crop_list, images_spatial_crop, tokenized_image, crop_ratio)\n",
        "        \"\"\"\n",
        "        images_list = []\n",
        "        images_crop_list = []\n",
        "        images_spatial_crop = []\n",
        "\n",
        "        if self.crop_mode:\n",
        "            # Determine crop ratio based on image size\n",
        "            if image.size[0] <= 640 and image.size[1] <= 640:\n",
        "                crop_ratio = (1, 1)\n",
        "                images_crop_raw = []\n",
        "            else:\n",
        "                images_crop_raw, crop_ratio = dynamic_preprocess(\n",
        "                    image, min_num=2, max_num=9,\n",
        "                    image_size=self.image_size, use_thumbnail=False\n",
        "                )\n",
        "\n",
        "            # Process global view with padding\n",
        "            global_view = ImageOps.pad(\n",
        "                image, (self.base_size, self.base_size),\n",
        "                color=tuple(int(x * 255) for x in self.image_transform.mean)\n",
        "            )\n",
        "            images_list.append(self.image_transform(global_view).to(self.dtype))\n",
        "\n",
        "            width_crop_num, height_crop_num = crop_ratio\n",
        "            images_spatial_crop.append([width_crop_num, height_crop_num])\n",
        "\n",
        "            # Process local views (crops) if applicable\n",
        "            if width_crop_num > 1 or height_crop_num > 1:\n",
        "                for crop_img in images_crop_raw:\n",
        "                    images_crop_list.append(\n",
        "                        self.image_transform(crop_img).to(self.dtype)\n",
        "                    )\n",
        "\n",
        "            # Calculate image tokens\n",
        "            num_queries = math.ceil((self.image_size // self.patch_size) / self.downsample_ratio)\n",
        "            num_queries_base = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "\n",
        "            tokenized_image = ([self.image_token_id] * num_queries_base + [self.image_token_id]) * num_queries_base\n",
        "            tokenized_image += [self.image_token_id]\n",
        "\n",
        "            if width_crop_num > 1 or height_crop_num > 1:\n",
        "                tokenized_image += ([self.image_token_id] * (num_queries * width_crop_num) + [self.image_token_id]) * (\n",
        "                    num_queries * height_crop_num)\n",
        "\n",
        "        else:  # crop_mode = False\n",
        "            crop_ratio = (1, 1)\n",
        "            images_spatial_crop.append([1, 1])\n",
        "\n",
        "            # For smaller base sizes, resize; for larger, pad\n",
        "            if self.base_size <= 640:\n",
        "                resized_image = image.resize((self.base_size, self.base_size), Image.LANCZOS)\n",
        "                images_list.append(self.image_transform(resized_image).to(self.dtype))\n",
        "            else:\n",
        "                global_view = ImageOps.pad(\n",
        "                    image, (self.base_size, self.base_size),\n",
        "                    color=tuple(int(x * 255) for x in self.image_transform.mean)\n",
        "                )\n",
        "                images_list.append(self.image_transform(global_view).to(self.dtype))\n",
        "\n",
        "            num_queries = math.ceil((self.base_size // self.patch_size) / self.downsample_ratio)\n",
        "            tokenized_image = ([self.image_token_id] * num_queries + [self.image_token_id]) * num_queries\n",
        "            tokenized_image += [self.image_token_id]\n",
        "\n",
        "        return images_list, images_crop_list, images_spatial_crop, tokenized_image, crop_ratio\n",
        "\n",
        "    def process_single_sample(self, messages: List[Dict]) -> Dict[str, Any]:\n",
        "            \"\"\"\n",
        "            Process a single conversation into model inputs.\n",
        "            \"\"\"\n",
        "\n",
        "            # --- 1. Setup ---\n",
        "            images = []\n",
        "            for message in messages:\n",
        "                if \"images\" in message and message[\"images\"]:\n",
        "                    for img_data in message[\"images\"]:\n",
        "                        if img_data is not None:\n",
        "                            pil_image = self.deserialize_image(img_data)\n",
        "                            images.append(pil_image)\n",
        "\n",
        "            if not images:\n",
        "                raise ValueError(\"No images found in sample. Please ensure all samples contain images.\")\n",
        "\n",
        "            tokenized_str = []\n",
        "            images_seq_mask = []\n",
        "            images_list, images_crop_list, images_spatial_crop = [], [], []\n",
        "\n",
        "            prompt_token_count = -1 # Index to start training\n",
        "            assistant_started = False\n",
        "            image_idx = 0\n",
        "\n",
        "            # Add BOS token at the very beginning\n",
        "            tokenized_str.append(self.bos_id)\n",
        "            images_seq_mask.append(False)\n",
        "\n",
        "            for message in messages:\n",
        "                role = message[\"role\"]\n",
        "                content = message[\"content\"]\n",
        "\n",
        "                # Check if this is the assistant's turn\n",
        "                if role == \"<|Assistant|>\":\n",
        "                    if not assistant_started:\n",
        "                        # This is the split point. All tokens added *so far*\n",
        "                        # are part of the prompt.\n",
        "                        prompt_token_count = len(tokenized_str)\n",
        "                        assistant_started = True\n",
        "\n",
        "                    # Append the EOS token string to the *end* of assistant content\n",
        "                    content = f\"{content.strip()} {self.tokenizer.eos_token}\"\n",
        "\n",
        "                # Split this message's content by the image token\n",
        "                text_splits = content.split('<image>')\n",
        "\n",
        "                for i, text_sep in enumerate(text_splits):\n",
        "                    # Tokenize the text part\n",
        "                    tokenized_sep = text_encode(self.tokenizer, text_sep, bos=False, eos=False)\n",
        "                    tokenized_str.extend(tokenized_sep)\n",
        "                    images_seq_mask.extend([False] * len(tokenized_sep))\n",
        "\n",
        "                    # If this text is followed by an <image> tag\n",
        "                    if i < len(text_splits) - 1:\n",
        "                        if image_idx >= len(images):\n",
        "                            raise ValueError(\n",
        "                                f\"Data mismatch: Found '<image>' token but no corresponding image.\"\n",
        "                            )\n",
        "\n",
        "                        # Process the image\n",
        "                        image = images[image_idx]\n",
        "                        img_list, crop_list, spatial_crop, tok_img, _ = self.process_image(image)\n",
        "\n",
        "                        images_list.extend(img_list)\n",
        "                        images_crop_list.extend(crop_list)\n",
        "                        images_spatial_crop.extend(spatial_crop)\n",
        "\n",
        "                        # Add image placeholder tokens\n",
        "                        tokenized_str.extend(tok_img)\n",
        "                        images_seq_mask.extend([True] * len(tok_img))\n",
        "\n",
        "                        image_idx += 1 # Move to the next image\n",
        "\n",
        "            # --- 3. Validation and Final Prep ---\n",
        "            if image_idx != len(images):\n",
        "                raise ValueError(\n",
        "                    f\"Data mismatch: Found {len(images)} images but only {image_idx} '<image>' tokens were used.\"\n",
        "                )\n",
        "\n",
        "            # If we never found an assistant message, we're in a weird state\n",
        "            # (e.g., user-only prompt). We mask everything.\n",
        "            if not assistant_started:\n",
        "                print(\"Warning: No assistant message found in sample. Masking all tokens.\")\n",
        "                prompt_token_count = len(tokenized_str)\n",
        "\n",
        "            # Prepare image tensors\n",
        "            images_ori = torch.stack(images_list, dim=0)\n",
        "            images_spatial_crop_tensor = torch.tensor(images_spatial_crop, dtype=torch.long)\n",
        "\n",
        "            if images_crop_list:\n",
        "                images_crop = torch.stack(images_crop_list, dim=0)\n",
        "            else:\n",
        "                images_crop = torch.zeros((1, 3, self.base_size, self.base_size), dtype=self.dtype)\n",
        "\n",
        "            return {\n",
        "                \"input_ids\": torch.tensor(tokenized_str, dtype=torch.long),\n",
        "                \"images_seq_mask\": torch.tensor(images_seq_mask, dtype=torch.bool),\n",
        "                \"images_ori\": images_ori,\n",
        "                \"images_crop\": images_crop,\n",
        "                \"images_spatial_crop\": images_spatial_crop_tensor,\n",
        "                \"prompt_token_count\": prompt_token_count, # This is now accurate\n",
        "            }\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Collate batch of samples\"\"\"\n",
        "        batch_data = []\n",
        "\n",
        "        # Process each sample\n",
        "        for feature in features:\n",
        "            try:\n",
        "                processed = self.process_single_sample(feature['messages'])\n",
        "                batch_data.append(processed)\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing sample: {e}\")\n",
        "                continue\n",
        "\n",
        "        if not batch_data:\n",
        "            raise ValueError(\"No valid samples in batch\")\n",
        "\n",
        "        # Extract lists\n",
        "        input_ids_list = [item['input_ids'] for item in batch_data]\n",
        "        images_seq_mask_list = [item['images_seq_mask'] for item in batch_data]\n",
        "        prompt_token_counts = [item['prompt_token_count'] for item in batch_data]\n",
        "\n",
        "        # Pad sequences\n",
        "        input_ids = pad_sequence(input_ids_list, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        images_seq_mask = pad_sequence(images_seq_mask_list, batch_first=True, padding_value=False)\n",
        "\n",
        "        # Create labels\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        # Mask padding tokens\n",
        "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
        "\n",
        "        # Mask image tokens (model shouldn't predict these)\n",
        "        labels[images_seq_mask] = -100\n",
        "\n",
        "        # Mask user prompt tokens when train_on_responses_only=True (only train on assistant responses)\n",
        "        if self.train_on_responses_only:\n",
        "            for idx, prompt_count in enumerate(prompt_token_counts):\n",
        "                if prompt_count > 0:\n",
        "                    labels[idx, :prompt_count] = -100\n",
        "\n",
        "        # Create attention mask\n",
        "        attention_mask = (input_ids != self.tokenizer.pad_token_id).long()\n",
        "\n",
        "        # Prepare images batch (list of tuples)\n",
        "        images_batch = []\n",
        "        for item in batch_data:\n",
        "            images_batch.append((item['images_crop'], item['images_ori']))\n",
        "\n",
        "        # Stack spatial crop info\n",
        "        images_spatial_crop = torch.cat([item['images_spatial_crop'] for item in batch_data], dim=0)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"labels\": labels,\n",
        "            \"images\": images_batch,\n",
        "            \"images_seq_mask\": images_seq_mask,\n",
        "            \"images_spatial_crop\": images_spatial_crop,\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8246dac"
      },
      "source": [
        "## Define Character Error Rate (CER) Metric\n",
        "\n",
        "### Subtask:\n",
        "Implement a function to calculate the Character Error Rate (CER), which will quantify the difference between the ground truth text and the model's predicted text. This metric will be crucial for evaluating the transcription accuracy of the models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2f75243"
      },
      "source": [
        "from jiwer import cer\n",
        "\n",
        "def calculate_cer(ground_truth, hypothesis):\n",
        "    \"\"\"Calculates the Character Error Rate (CER) between two strings.\"\"\"\n",
        "    return cer(ground_truth, hypothesis)\n",
        "\n",
        "print(\"CER calculation function defined.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`. We also support TRL's `DPOTrainer`!\n",
        "\n",
        "We use our new `DeepSeekOCRDataCollator` which will help in our vision finetuning setup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "from unsloth import is_bf16_supported\n",
        "FastVisionModel.for_training(finetuned_model) # Enable for training!\n",
        "data_collator = DeepSeekOCRDataCollator(\n",
        "    tokenizer = original_tokenizer,\n",
        "    model = finetuned_model,\n",
        "    image_size = 640,\n",
        "    base_size = 1024,\n",
        "    crop_mode = True,\n",
        "    train_on_responses_only = True,\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model = finetuned_model,\n",
        "    tokenizer = original_tokenizer,\n",
        "    data_collator = data_collator, # Must use!\n",
        "    train_dataset = train_dataset,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 5,\n",
        "        warmup_steps = 5,\n",
        "        # max_steps = 60,\n",
        "        num_train_epochs = 1, # Set this instead of max_steps for full training runs\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        fp16 = not is_bf16_supported(),  # Use fp16 if bf16 is not supported\n",
        "        bf16 = is_bf16_supported(),  # Use bf16 if supported\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"none\",     # For Weights and Biases\n",
        "        dataloader_num_workers=2,\n",
        "        # You MUST put the below items for vision finetuning:\n",
        "        remove_unused_columns = False,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2ejIt2xSNKKp"
      },
      "outputs": [],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "pCqnaKmlO1U9"
      },
      "outputs": [],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Assign original tokenizer -> finetuned tokenizer after training\n",
        "finetuned_tokenizer = original_tokenizer"
      ],
      "metadata": {
        "id": "W1TpcV_OpIwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q"
      },
      "outputs": [],
      "source": [
        "prompt = DEEPSEEK_PROMPT\n",
        "image_file = 'your_image.jpg'\n",
        "output_path = 'your/output/dir'\n",
        "# Tiny: base_size = 512, image_size = 512, crop_mode = False\n",
        "# Small: base_size = 640, image_size = 640, crop_mode = False\n",
        "# Base: base_size = 1024, image_size = 1024, crop_mode = False\n",
        "# Large: base_size = 1280, image_size = 1280, crop_mode = False\n",
        "\n",
        "# Gundam: base_size = 1024, image_size = 640, crop_mode = True\n",
        "\n",
        "res = finetuned_model.infer(original_tokenizer, prompt=prompt, image_file=image_file,\n",
        "    output_path = output_path,\n",
        "    image_size=640,\n",
        "    base_size=1024,\n",
        "    crop_mode=True,\n",
        "    save_results = True,\n",
        "    test_compress = False,\n",
        "    eval_mode=False) # no need to return anything at this place\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "finetuned_model.save_pretrained(\"lora_model\")  # Local saving\n",
        "finetuned_tokenizer.save_pretrained(\"lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    from unsloth import FastVisionModel\n",
        "    model, tokenizer = FastVisionModel.from_pretrained(\n",
        "        model_name = \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = False, # Use 4bit to reduce memory use. False for 16bit LoRA.\n",
        "        auto_model = AutoModel,\n",
        "        trust_remote_code=True,\n",
        "        unsloth_force_compile=True,\n",
        "        use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for long context\n",
        "    )\n",
        "    FastVisionModel.for_inference(model) # Enable for inference!\n",
        "\n",
        "prompt = DEEPSEEK_PROMPT\n",
        "image_file = 'your_image.jpg'\n",
        "output_path = 'your/output/dir'\n",
        "\n",
        "# Tiny: base_size = 512, image_size = 512, crop_mode = False\n",
        "# Small: base_size = 640, image_size = 640, crop_mode = False\n",
        "# Base: base_size = 1024, image_size = 1024, crop_mode = False\n",
        "# Large: base_size = 1280, image_size = 1280, crop_mode = False\n",
        "\n",
        "# Gundam: base_size = 1024, image_size = 640, crop_mode = True\n",
        "\n",
        "res = finetuned_model.infer(original_tokenizer, prompt=prompt, image_file=image_file,\n",
        "    output_path = output_path,\n",
        "    image_size=640,\n",
        "    base_size=1024,\n",
        "    crop_mode=True,\n",
        "    save_results = True,\n",
        "    test_compress = False,\n",
        "    eval_mode=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f422JgM9sdVT"
      },
      "source": [
        "### Saving to float16 for VLLM\n",
        "\n",
        "We also support saving to `float16` directly. Select `merged_16bit` for float16. Use `push_to_hub_merged` to upload to your Hugging Face account! You can go to https://huggingface.co/settings/tokens for your personal tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iHjt_SMYsd3P"
      },
      "outputs": [],
      "source": [
        "# Select ONLY 1 to save! (Both not needed!)\n",
        "\n",
        "# Save locally to 16bit\n",
        "if False: model.save_pretrained_merged(\"unsloth_finetune\", tokenizer,)\n",
        "\n",
        "# To export and save to your Hugging Face account\n",
        "if False: model.push_to_hub_merged(\"YOUR_USERNAME/unsloth_finetune\", tokenizer, token = \"PUT_HERE\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1f7f91e7"
      },
      "source": [
        "import os\n",
        "# Define the folder to zip and the name of the zip file\n",
        "folder_to_zip = \"lora_model\"\n",
        "output_zip_file = f\"{folder_to_zip}.zip\"\n",
        "\n",
        "# Use the !zip command to compress the folder\n",
        "!zip -r {output_zip_file} {folder_to_zip}\n",
        "\n",
        "print(f\"Successfully created {output_zip_file}\")\n",
        "print(f\"You can download it from the files tab on the left (if using Colab) or locate it in the current directory.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip lora_model.zip\n",
        "print(\"Unzipping successfully!\")"
      ],
      "metadata": {
        "id": "WtdGkaKuvYIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7caf1a8"
      },
      "source": [
        "from unsloth import FastVisionModel\n",
        "from transformers import AutoModel\n",
        "\n",
        "# Reload the fine-tuned DeepSeek-OCR model with 4-bit quantization\n",
        "finetuned_model, finetuned_tokenizer = FastVisionModel.from_pretrained(\n",
        "    model_name = \"lora_model\",\n",
        "    load_in_4bit = True, # Use 4bit quantization\n",
        "    auto_model = AutoModel,\n",
        "    trust_remote_code=True,\n",
        "    unsloth_force_compile=True,\n",
        "    use_gradient_checkpointing = False, # Not needed for inference\n",
        ")\n",
        "\n",
        "print(\"Fine-tuned DeepSeek-OCR model and tokenizer reloaded with 4-bit quantization.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5006861"
      },
      "source": [
        "import os\n",
        "\n",
        "test_samples_for_inference = []\n",
        "\n",
        "for i, sample in enumerate(test_dataset):\n",
        "    ground_truth_text = sample['messages'][1]['content']\n",
        "    image_pil = sample['messages'][0]['images'][0]\n",
        "\n",
        "    # Create a unique temporary filename\n",
        "    temp_filename = f'temp_image_{i}.png'\n",
        "\n",
        "    # Save the PIL image to the temporary file\n",
        "    image_pil.save(temp_filename)\n",
        "\n",
        "    test_samples_for_inference.append({\n",
        "        'image_path': temp_filename,\n",
        "        'ground_truth': ground_truth_text,\n",
        "        'image_pil': image_pil # Keep the PIL object for potential future use or display\n",
        "    })\n",
        "\n",
        "print(f\"Prepared {len(test_samples_for_inference)} test samples for inference, saving images to temporary files.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Baseline and Finetuned Model"
      ],
      "metadata": {
        "id": "qUNejZd5u8pW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import collections\n",
        "# The following variables are assumed to be defined in your notebook environment:\n",
        "# original_model, original_tokenizer\n",
        "# finetuned_model, finetuned_tokenizer\n",
        "# calculate_cer(ground_truth, prediction)\n",
        "# test_samples_for_inference (list of dicts, each having image_path, ground_truth, and data_type)\n",
        "\n",
        "\n",
        "# --- 1. Configuration and Setup ---\n",
        "# Create the output directory for illustrative results (input/output)\n",
        "EVALUATION_DIR = os.path.join(OUTPUT_DIR, 'evaluation_results')\n",
        "os.makedirs(EVALUATION_DIR, exist_ok=True)\n",
        "\n",
        "# Define the data types for stratified evaluation (assumed)\n",
        "DATA_TYPES = [\"printed_text\", \"handwriting\", \"tables\", \"forms\"]\n",
        "\n",
        "# Initialize structure to store results and CERs for both models\n",
        "results = {\n",
        "    'original': {'predictions': [], 'cers': [], 'metrics_by_type': collections.defaultdict(lambda: {'cers': [], 'samples': []})},\n",
        "    'finetuned': {'predictions': [], 'cers': [], 'metrics_by_type': collections.defaultdict(lambda: {'cers': [], 'samples': []})}\n",
        "}\n",
        "\n",
        "\n",
        "# --- 2. Utility Function: Run Inference ---\n",
        "def run_inference(model, tokenizer, sample, prompt=DEEPSEEK_PROMPT, output_path=EVALUATION_DIR):\n",
        "    \"\"\"Performs inference using the custom model.infer() method.\"\"\"\n",
        "    image_file = sample['image_path']\n",
        "    try:\n",
        "        prediction = model.infer(\n",
        "            tokenizer,\n",
        "            prompt=prompt,\n",
        "            image_file=image_file,\n",
        "            output_path=output_path,\n",
        "            eval_mode=True # Assumes infer() is modified to return the output when eval_mode=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Inference error: {e}\")\n",
        "        prediction = \"\" # Fallback\n",
        "\n",
        "    # Normalize output\n",
        "    if not isinstance(prediction, str):\n",
        "        prediction = \"\"\n",
        "    if hasattr(tokenizer, 'eos_token'):\n",
        "        prediction = prediction.replace(tokenizer.eos_token, \"\").strip()\n",
        "    else:\n",
        "        prediction = prediction.strip()\n",
        "\n",
        "    return prediction\n",
        "\n",
        "\n",
        "# --- 3. Main Loop: Run Evaluation ---\n",
        "print(\"Starting comprehensive evaluation...\")\n",
        "\n",
        "for i, sample in enumerate(test_samples_for_inference):\n",
        "\n",
        "    # 3.1. Sample Classification\n",
        "    data_type = sample.get('data_type', 'unspecified')\n",
        "    if data_type not in DATA_TYPES:\n",
        "        data_type = 'unspecified' # Handle case of unknown data type\n",
        "\n",
        "    # 3.2. Run Original Model\n",
        "    orig_pred = run_inference(original_model, original_tokenizer, sample)\n",
        "    orig_cer = calculate_cer(sample['ground_truth'], orig_pred)\n",
        "\n",
        "    results['original']['predictions'].append(orig_pred)\n",
        "    results['original']['cers'].append(orig_cer)\n",
        "    results['original']['metrics_by_type'][data_type]['cers'].append(orig_cer)\n",
        "\n",
        "    # 3.3. Run Fine-tuned Model\n",
        "    ft_pred = run_inference(finetuned_model, finetuned_tokenizer, sample)\n",
        "    ft_cer = calculate_cer(sample['ground_truth'], ft_pred)\n",
        "\n",
        "    results['finetuned']['predictions'].append(ft_pred)\n",
        "    results['finetuned']['cers'].append(ft_cer)\n",
        "    results['finetuned']['metrics_by_type'][data_type]['cers'].append(ft_cer)\n",
        "\n",
        "    # 3.4. Store Sample for Error Analysis and Illustration\n",
        "    sample_result = {\n",
        "        'ground_truth': sample['ground_truth'],\n",
        "        'original_prediction': orig_pred,\n",
        "        'finetuned_prediction': ft_pred,\n",
        "        'original_cer': orig_cer,\n",
        "        'finetuned_cer': ft_cer,\n",
        "        'image_path': sample['image_path'],\n",
        "        'index': i\n",
        "    }\n",
        "\n",
        "    # Store sample for analysis (error analysis)\n",
        "    # We store the same result object under both models for comparison\n",
        "    results['original']['metrics_by_type'][data_type]['samples'].append(sample_result)\n",
        "    results['finetuned']['metrics_by_type'][data_type]['samples'].append(sample_result)\n",
        "\n",
        "    if (i + 1) % 50 == 0:\n",
        "        print(f\"Processed {i + 1}/{len(test_samples_for_inference)} samples.\")\n",
        "\n",
        "\n",
        "# --- 4. Calculate and Print Overall Results ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"             OVERALL EVALUATION SUMMARY\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Overall CER\n",
        "avg_orig_cer = np.mean(results['original']['cers']) if results['original']['cers'] else 0\n",
        "avg_ft_cer = np.mean(results['finetuned']['cers']) if results['finetuned']['cers'] else 0\n",
        "\n",
        "print(f\"Total evaluated samples: {len(test_samples_for_inference)}\")\n",
        "print(f\"Average CER (Original Model): {avg_orig_cer:.4f}\")\n",
        "print(f\"Average CER (Fine-tuned Model): {avg_ft_cer:.4f}\")\n",
        "\n",
        "# Comparison\n",
        "improvement = avg_orig_cer - avg_ft_cer\n",
        "print(f\"Absolute CER Improvement: {improvement:+.4f}\")\n",
        "print(f\"The Fine-tuned model {'Improved' if improvement > 0 else 'Degraded'} performance.\" if improvement != 0 else \"Performance is similar.\")\n",
        "\n",
        "\n",
        "# --- 5. Evaluation by Data Type (Stratified Analysis) ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"             EVALUATION BY DATA TYPE\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for dt in DATA_TYPES:\n",
        "    orig_cers = results['original']['metrics_by_type'][dt]['cers']\n",
        "    ft_cers = results['finetuned']['metrics_by_type'][dt]['cers']\n",
        "\n",
        "    if orig_cers:\n",
        "        avg_orig_dt_cer = np.mean(orig_cers)\n",
        "        avg_ft_dt_cer = np.mean(ft_cers)\n",
        "        dt_improvement = avg_orig_dt_cer - avg_ft_dt_cer\n",
        "\n",
        "        print(f\"\\nData Type: {dt.upper()} (Samples: {len(orig_cers)})\")\n",
        "        print(f\"  - Original CER: {avg_orig_dt_cer:.4f}\")\n",
        "        print(f\"  - Fine-tuned CER: {avg_ft_dt_cer:.4f}\")\n",
        "        print(f\"  - Change: {dt_improvement:+.4f} ({'Improved' if dt_improvement > 0 else 'Degraded'})\")\n",
        "\n",
        "\n",
        "# --- 6. Error Analysis and Illustration ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"         ERROR ANALYSIS AND EXAMPLES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Find examples of significant improvement and degradation\n",
        "significant_improvement_samples = []\n",
        "significant_degradation_samples = []\n",
        "\n",
        "for dt in DATA_TYPES:\n",
        "    for sample_result in results['finetuned']['metrics_by_type'][dt]['samples']:\n",
        "        # Significant improvement (FT CER is at least 0.5 lower than Original CER)\n",
        "        if sample_result['original_cer'] > 0.5 and sample_result['original_cer'] - sample_result['finetuned_cer'] >= 0.5:\n",
        "            significant_improvement_samples.append((dt, sample_result))\n",
        "\n",
        "        # Significant degradation (FT CER is at least 0.5 higher than Original CER)\n",
        "        if sample_result['finetuned_cer'] > 0.5 and sample_result['finetuned_cer'] - sample_result['original_cer'] >= 0.5:\n",
        "            significant_degradation_samples.append((dt, sample_result))\n",
        "\n",
        "# Print illustrative examples\n",
        "def print_sample_details(title, samples):\n",
        "    print(f\"\\n--- {title} (Total: {len(samples)} samples) ---\")\n",
        "    if not samples:\n",
        "        print(\"No notable examples found.\")\n",
        "        return\n",
        "\n",
        "    # Select the first 3 examples (or randomly)\n",
        "    selected_samples = samples[:3]\n",
        "\n",
        "    for dt, sample in selected_samples:\n",
        "        print(f\"\\nSample #{sample['index']} ({dt.upper()}) | Image: {os.path.basename(sample['image_path'])}\")\n",
        "        print(f\"  - Ground Truth (GT): {sample['ground_truth']}\")\n",
        "        print(f\"  - Original (CER {sample['original_cer']:.4f}): {sample['original_prediction']}\")\n",
        "        print(f\"  - Fine-tuned (CER {sample['finetuned_cer']:.4f}): {sample['finetuned_prediction']}\")\n",
        "\n",
        "        # Error analysis notes\n",
        "        if sample['original_cer'] > 0 and sample['finetuned_cer'] == 0:\n",
        "            print(\"  >>> Analysis: FT completely resolved the error.\")\n",
        "        elif sample['original_cer'] == 0 and sample['finetuned_cer'] > 0:\n",
        "            print(\"  >>> Analysis: FT introduced an error (regression).\")\n",
        "        elif sample['original_cer'] > sample['finetuned_cer']:\n",
        "            print(\"  >>> Analysis: FT improved accuracy, reducing errors.\")\n",
        "        elif sample['finetuned_cer'] > sample['original_cer']:\n",
        "            print(\"  >>> Analysis: FT degraded accuracy compared to Original.\")\n",
        "\n",
        "\n",
        "print_sample_details(\"Significant Improvement Examples (FT > Original)\", significant_improvement_samples)\n",
        "print_sample_details(\"Significant Degradation Examples (FT < Original)\", significant_degradation_samples)\n",
        "\n",
        "\n",
        "# --- 7. Cleanup ---\n",
        "# Note: If you want to keep the image files to view the error analysis, skip this section\n",
        "# for sample in test_samples_for_inference:\n",
        "#     if os.path.exists(sample['image_path']):\n",
        "#         os.remove(sample['image_path'])\n",
        "# print(\"\\nCleaned up temporary image files.\")\n",
        "\n",
        "print(\"\\nComprehensive evaluation complete.\")"
      ],
      "metadata": {
        "id": "AEdZEqCjMRve"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2a0543da62e84b268f4d3bb97452609c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59d29e498f2d4aa4aad7706324d55e03",
              "IPY_MODEL_f049f92b4b434a8893e7c6be5bfc0545",
              "IPY_MODEL_cc21e78711624a42b1d914f5e157a310"
            ],
            "layout": "IPY_MODEL_684ccf2f7f934aa8aabfa4fcff16235a"
          }
        },
        "59d29e498f2d4aa4aad7706324d55e03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15a7f8f0065647e4995b1b6b52e7bc8e",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2ba4665432b64573b87f2611ef903802",
            "value": "Fetchingâ€‡21â€‡files:â€‡100%"
          }
        },
        "f049f92b4b434a8893e7c6be5bfc0545": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f5f9e13cdda9434eb205bcfc8823e6ae",
            "max": 21,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b26c0b53d7a49768b85f782c1c20874",
            "value": 21
          }
        },
        "cc21e78711624a42b1d914f5e157a310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_187c66983ec24d8ba95a60af28f6c467",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_d92b944f355e4a98a4ef54eb6360c893",
            "value": "â€‡21/21â€‡[00:00&lt;00:00,â€‡545.89it/s]"
          }
        },
        "684ccf2f7f934aa8aabfa4fcff16235a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15a7f8f0065647e4995b1b6b52e7bc8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ba4665432b64573b87f2611ef903802": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f5f9e13cdda9434eb205bcfc8823e6ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b26c0b53d7a49768b85f782c1c20874": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "187c66983ec24d8ba95a60af28f6c467": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d92b944f355e4a98a4ef54eb6360c893": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64b88451471f41fab2c3675e9d07e7e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_79f06f9e152a4f308f94e5e8f9de3cfe",
              "IPY_MODEL_7378120267234b6485e36b9c207c8937",
              "IPY_MODEL_6210f0ed6dab4f70a58b505df0c960b3"
            ],
            "layout": "IPY_MODEL_f815c30ee1ca4557a3dd57cda596b9b4"
          }
        },
        "79f06f9e152a4f308f94e5e8f9de3cfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b18b7a7475964cb491a15ce08e1c4315",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_8b3dd99568084b6ab8448b52524d858a",
            "value": "Generatingâ€‡trainâ€‡split:â€‡"
          }
        },
        "7378120267234b6485e36b9c207c8937": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef986553433944ffa505e56c6fc55485",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0fa784ecaa68483b8b4a24bb235547bd",
            "value": 1
          }
        },
        "6210f0ed6dab4f70a58b505df0c960b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00286b23f2c24eee8c57a9fec06e8183",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_efd7ad991d8046f6a3182cdcd61d4619",
            "value": "â€‡4368/0â€‡[00:00&lt;00:00,â€‡29915.29â€‡examples/s]"
          }
        },
        "f815c30ee1ca4557a3dd57cda596b9b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b18b7a7475964cb491a15ce08e1c4315": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b3dd99568084b6ab8448b52524d858a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef986553433944ffa505e56c6fc55485": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "0fa784ecaa68483b8b4a24bb235547bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "00286b23f2c24eee8c57a9fec06e8183": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efd7ad991d8046f6a3182cdcd61d4619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2dd252dc38c74311882f785030865e95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_de3e1f7404d741b1aefa4df3b4d01d72",
              "IPY_MODEL_b7aa55894d734197bdd5bc638008ff46",
              "IPY_MODEL_2615ada100ad4afaa058cfc939ed2140"
            ],
            "layout": "IPY_MODEL_d267935cdad9441ca33764b4b153e7b5"
          }
        },
        "de3e1f7404d741b1aefa4df3b4d01d72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b86ab892923435ba0cf2da603157c0c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5195ecaa378a4206841b70d6ac22a227",
            "value": "Generatingâ€‡trainâ€‡split:â€‡"
          }
        },
        "b7aa55894d734197bdd5bc638008ff46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f521159dc8945d183819d70a7a2220f",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_815ea3d114094caba231aa1e299eecf0",
            "value": 1
          }
        },
        "2615ada100ad4afaa058cfc939ed2140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee717c91473f4a6eaa21cf802ad84e6c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c99a754f7b9642729a0423f6508bd890",
            "value": "â€‡201/0â€‡[00:00&lt;00:00,â€‡2482.70â€‡examples/s]"
          }
        },
        "d267935cdad9441ca33764b4b153e7b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b86ab892923435ba0cf2da603157c0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5195ecaa378a4206841b70d6ac22a227": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f521159dc8945d183819d70a7a2220f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "815ea3d114094caba231aa1e299eecf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee717c91473f4a6eaa21cf802ad84e6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c99a754f7b9642729a0423f6508bd890": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}